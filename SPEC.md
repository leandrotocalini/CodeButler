# CodeButler â€” Spec

---

## 1. What is CodeButler

CodeButler is **a multi-agent AI dev team accessible from Slack**. One Go binary, multiple agents, each with its own personality, context, and memory â€” all parameterized from the same code. You describe what you want in a Slack thread. A cheap agent (the PM) plans the work, explores the codebase, and proposes a plan. You approve. The Coder agent executes â€” with a full agent loop, tool use, file editing, test running, and PR creation. At close, the Lead agent mediates a retrospective between all agents to improve workflows. No terminal needed. You can be on your phone.

### 1.1 Process Model: Separate Processes, Goroutines Per Thread

**Six independent OS processes**, one per agent. Same Go binary, parameterized by `--role`:

```bash
codebutler --role pm          # always running, listens for Slack
codebutler --role coder       # always running, listens for @codebutler.coder
codebutler --role reviewer    # always running, listens for @codebutler.reviewer
codebutler --role researcher  # always running, listens for @codebutler.researcher
codebutler --role lead        # always running, listens for @codebutler.lead
codebutler --role artist      # always running, listens for @codebutler.artist
```

**Each process:**
1. Connects to Slack via Socket Mode (its own listener)
2. Filters messages: only responds to @mentions directed at it (or user messages for PM)
3. Maintains a **thread registry** (`map[string]*ThreadWorker`) â€” one goroutine per active thread
4. Executes tools locally in its own process (Read, Write, Bash, etc.)
5. Reads its own MD + `global.md` as system prompt
6. Calls OpenRouter with its configured model

**Communication between agents is 100% via Slack messages.** No IPC, no RPC, no shared memory. When PM needs Coder, it posts `@codebutler.coder implement...` in the thread. The Coder process picks it up from its Slack listener. Same for all agent-to-agent communication.

**No shared database.** The Slack thread is the source of truth for inter-agent communication â€” what agents say to each other and the user. But each agent also maintains a **local conversation file** in the thread's worktree (`conversations/<role>.json`). This file holds the full back-and-forth with the model: system prompt, tool calls, tool results, intermediate reasoning â€” most of which never appears in Slack. The model returns many things (tool calls, partial thoughts, retries) that the agent processes internally; only the final curated output gets posted to the thread.

The worktree already maps 1:1 to the thread (via the branch), so conversation files just live there â€” no separate thread-id directory needed. On restart, agents read active threads from Slack to find unprocessed @mentions, and resume model conversations from the worktree's JSON files. No SQLite needed.

Same agent loop in every process (system prompt â†’ LLM call â†’ tool use â†’ execute â†’ append â†’ repeat), different parameters:
- **System prompt** â€” from `<role>.md` + `global.md`. One file per agent that IS the system prompt and evolves with learnings
- **Model** â€” from per-repo config (Kimi for PM, Opus for Coder, Sonnet for others)
- **Tool permissions** â€” behavioral (system prompt says what to use), not structural

**The PM is the entry point** â€” it handles all user messages. Other agents are idle until @mentioned. **Mediation:** when agents disagree, they escalate to the Lead. If the Lead can't resolve it, it asks the user.

### 1.2 The Six Agents

| Agent | What it does | Model | Writes code? | Cost |
|-------|-------------|-------|-------------|------|
| **PM** | Plans, explores codebase, orchestrates, talks to user | Kimi / GPT-4o-mini / Claude (swappable) | Never | ~$0.001/msg |
| **Researcher** | Subagent: web research on demand (spawned, returns, dies) | Cheap model (same tier as PM) | Never | ~$0.001/search |
| **Artist** | UI/UX designer + image generation | Claude Sonnet (UX reasoning) + OpenAI gpt-image-1 (images) | Never | ~$0.02-0.10/task |
| **Coder** | Writes code, runs tests, creates PRs | Claude Opus 4.6 | Always | ~$0.30-2.00/task |
| **Reviewer** | Code review: quality, security, tests, plan compliance | Claude Sonnet | Never | ~$0.02-0.10/review |
| **Lead** | Thread retrospective: mediates, evolves workflows and agent MDs | Claude Sonnet | Never | ~$0.01-0.05/thread |

All agents share the same tool set. Separation is behavioral via system prompts: the PM explores + orchestrates, the Researcher searches the web, the Artist designs UI/UX, the Coder builds, the Reviewer checks quality, the Lead mediates. Only the user approves.

### 1.3 End-to-End Flow

PM creates worktree (conversation persistence from the start) â†’ classifies intent â†’ selects workflow from `workflows.md` â†’ interviews user â†’ explores codebase â†’ spawns Researcher for web research if needed â†’ sends to Artist for UI/UX design if feature has visual component â†’ proposes plan (with Artist design) â†’ user approves â†’ sends plan + Artist design to Coder â†’ Coder implements + creates PR â†’ Reviewer reviews diff (loop with Coder until approved) â†’ Lead runs retrospective (discusses with agents, proposes learnings) â†’ user approves learnings â†’ merge PR â†’ cleanup.

For discovery: PM interviews â†’ Artist designs UX for visual features â†’ Lead builds roadmap in `.codebutler/roadmap.md`. Each roadmap item â†’ future implement thread, or `develop` for all at once.

For learn (onboarding): all agents explore codebase in parallel from their perspective â†’ populate project maps + `global.md` â†’ user approves.

### 1.4 Architecture: OpenRouter + Native Tools

**All LLM calls go through OpenRouter.** CodeButler implements the full agent loop natively in Go â€” no `claude` CLI, no subprocess. Each agent is the same runtime with different config.

All tools (Read, Write, Edit, Bash, Grep, Glob, WebSearch, WebFetch, GitCommit, GitPush, GHCreatePR, SendMessage, Research, GenerateImage, etc.) are implemented natively. The Artist is dual-model: Claude Sonnet via OpenRouter for UX reasoning + OpenAI gpt-image-1 directly for image generation.

### 1.5 Agent MDs (System Prompt = Memory)

Each agent has **one MD file** in `<repo>/.codebutler/` that is both its system prompt and its evolving memory. Seeded with defaults on first run, then the Lead appends learnings after each PR â€” only to agents that need them.

**Each agent MD has three sections:**
1. **Personality + rules** â€” behavioral instructions, tool permissions (seeded, rarely changes)
2. **Project map** â€” the project from that agent's perspective (evolves as the project grows)
3. **Behavioral learnings** â€” how to work better, interact with other agents, avoid past mistakes (from Lead retrospectives or direct user feedback)

This is how agents stay coherent â€” the Artist never proposes UX wildly different from what exists because its MD contains the current UI state. The Coder knows the conventions because they're in its MD.

Plus two shared files all agents read: `global.md` (shared project knowledge: architecture, tech stack, conventions) and `workflows.md` (process playbook).

### 1.6 MCP â€” Model Context Protocol

MCP lets agents use external tools beyond the built-in set. An MCP server is a child process that exposes tools over stdio â€” database queries, API calls, file system extensions, whatever the server implements. CodeButler's agent loop already does tool-calling; MCP tools appear alongside native tools in the same loop.

**Config:** `.codebutler/mcp.json` (per-repo, committed to git). Defines available servers and which agents can use them.

```json
{
  "servers": {
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": { "GITHUB_PERSONAL_ACCESS_TOKEN": "${GITHUB_TOKEN}" },
      "roles": ["pm", "coder", "reviewer", "lead"]
    },
    "postgres": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-postgres"],
      "env": { "DATABASE_URL": "${DATABASE_URL}" },
      "roles": ["coder", "reviewer"]
    },
    "linear": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-linear"],
      "env": { "LINEAR_API_KEY": "${LINEAR_API_KEY}" },
      "roles": ["pm", "lead"]
    },
    "figma": {
      "command": "npx",
      "args": ["-y", "@anthropic/mcp-server-figma"],
      "env": { "FIGMA_TOKEN": "${FIGMA_TOKEN}" },
      "roles": ["artist"]
    },
    "sentry": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sentry"],
      "env": { "SENTRY_AUTH_TOKEN": "${SENTRY_AUTH_TOKEN}" },
      "roles": ["coder", "reviewer", "pm"]
    }
  }
}
```

**Per-agent access (`roles` field):** each server lists which agents can use it. When an agent process starts, it only launches the MCP servers assigned to its role. The Coder gets database access; the PM doesn't. The Artist gets Figma; the Coder doesn't. If `roles` is omitted, all agents get access.

**Environment variables:** MCP server configs can reference env vars with `${VAR}` syntax. Secrets come from the environment (set in the service unit or shell), never stored in the committed config file.

**Lifecycle:**
1. Agent process starts â†’ reads `mcp.json` â†’ filters servers by its role â†’ launches each as a child process (stdio transport)
2. Agent discovers tools from each server (MCP `tools/list`)
3. MCP tools are added to the tool list sent to the LLM alongside native tools
4. When the LLM calls an MCP tool â†’ agent routes the call to the right server process â†’ returns result to LLM
5. Agent process stops â†’ all child MCP server processes are killed

**What this enables:**
- **GitHub:** PM triages issues, Reviewer posts inline PR comments, Lead links retrospective findings to issues â€” all via GitHub API, not just `gh` CLI
- **Databases:** Coder queries schemas, checks data, runs migrations
- **Project management:** PM reads/updates Linear or Jira tickets, Lead creates issues from retrospectives
- **Design:** Artist pulls components and styles from Figma
- **Monitoring:** Coder and Reviewer check Sentry for error context when debugging
- **Custom servers:** any stdio MCP server works â€” teams can build project-specific servers

**What this does NOT replace:** native tools (Read, Write, Bash, Grep, etc.) stay native. MCP is for external integrations, not for reimplementing built-in capabilities.

### 1.7 Skills â€” Custom Commands

Skills are project-specific, reusable commands defined as markdown files. While workflows define multi-step processes (implement, bugfix, discover), skills are more atomic â€” focused actions that teams use repeatedly. Think of them as custom slash commands with full agent backing.

**Where they live:** `.codebutler/skills/` â€” one `.md` file per skill, committed to git.

**Example skill â€” `deploy.md`:**

```markdown
# deploy

Deploy the project to an environment.

## Trigger
deploy, deploy to {environment}

## Agent
coder

## Prompt
Deploy the project to {{environment | default: "staging"}}.

1. Run the test suite first. If tests fail, stop and report
2. Run `./scripts/deploy.sh {{environment}}`
3. Verify the deployment is healthy (curl the /health endpoint)
4. Report: deployed version, URL, any warnings
```

**Example skill â€” `db-migrate.md`:**

```markdown
# db-migrate

Run database migrations.

## Trigger
migrate, run migrations, db migrate

## Agent
coder

## Prompt
Run database migrations.

1. Check for pending migrations: `npx prisma migrate status`
2. If pending: run `npx prisma migrate deploy`
3. If dev environment: run `npx prisma generate` to update client
4. Report: migrations applied, current state
```

**Example skill â€” `changelog.md`:**

```markdown
# changelog

Generate a changelog entry for the latest release.

## Trigger
changelog, what changed, release notes

## Agent
pm

## Prompt
Generate a changelog entry from recent git history.

1. Read the latest tag: `git describe --tags --abbrev=0`
2. List commits since that tag: `git log <tag>..HEAD --oneline`
3. Group by type (features, fixes, refactors)
4. Write a structured changelog entry in CHANGELOG.md
5. Post the entry in the thread for review
```

**Skill file format:**

| Section | Required | Description |
|---------|----------|-------------|
| `# name` | Yes | Skill name (matches filename) |
| Description | Yes | One-line description (shown in help) |
| `## Trigger` | Yes | Comma-separated trigger phrases. `{param}` captures variables |
| `## Agent` | Yes | Which agent executes: `pm`, `coder`, `reviewer`, `researcher`, `artist`, `lead` |
| `## Prompt` | Yes | The instructions sent to the agent. `{{param}}` resolves captured variables. `{{param \| default: "value"}}` for defaults |

**How it works:**

1. User posts in Slack: "deploy to production"
2. PM classifies intent â€” checks workflows first, then scans skills (trigger match)
3. PM resolves variables: `{environment}` â†’ `"production"`
4. PM routes to the skill's target agent with the resolved prompt
5. Agent executes following the prompt instructions
6. If the skill triggers code changes â†’ normal flow continues (PR, Reviewer, Lead)
7. If no code changes â†’ agent reports result in the thread, done

**Skills vs workflows:**

| | Workflows | Skills |
|---|-----------|--------|
| Scope | Multi-agent, multi-step processes | Single-agent, focused actions |
| Definition | `workflows.md` (one file, all workflows) | `skills/*.md` (one file per skill) |
| Who creates | Seeded on init, evolved by Lead | Seeded on init + team members + Lead |
| Parameters | None (PM interviews for context) | Captured from trigger phrases |
| Examples | implement, bugfix, discover, refactor | explain, test, hotfix, changelog, docs, security-scan, status |

**PM intent classification order:**
1. Exact workflow match â†’ run workflow
2. Skill trigger match â†’ run skill
3. Ambiguous â†’ present options (workflows + skills) to user

**Seeded skills** (included on `codebutler init`):

| Skill | Agent | What it does |
|-------|-------|-------------|
| `explain` | PM | Explain how a part of the codebase works |
| `test` | Coder | Write tests for a specific file, function, or module |
| `changelog` | PM | Generate a changelog entry from recent git history |
| `hotfix` | PM | Investigate and quick-fix a bug (PM finds root cause, then sends to Coder) |
| `docs` | Coder | Generate or update documentation for a part of the codebase |
| `security-scan` | Reviewer | Run a security audit (OWASP Top 10 + project-specific risks) |
| `self-document` | Lead | Document recent work in JOURNEY.md |
| `status` | PM | Report project status (roadmap, active PRs, recent activity) |
| `triage-issue` | PM | Triage a GitHub issue: analyze, label, prioritize, route (GitHub MCP) |
| `review-pr` | Reviewer | Review a PR with inline comments on GitHub (GitHub MCP) |
| `release` | PM | Create a GitHub release with auto-generated notes (GitHub MCP) |

These live in `seeds/skills/` and are copied to `.codebutler/skills/` on init. Teams can modify, delete, or add new ones.

Skills marked **(GitHub MCP)** require the GitHub MCP server configured in `mcp.json`. They use GitHub API tools (`get_issue`, `create_pull_request_review`, `create_release`, etc.) for richer interaction than `gh` CLI alone â€” inline PR comments, issue labeling, release publishing.

**Who creates skills:**
- **Seeded** â€” 11 default skills cover common development tasks and GitHub workflows
- **Team members** â€” create `.md` files in `skills/` manually for project-specific operations
- **Lead** â€” proposes new skills during retrospective when it spots recurring patterns ("you've asked for deploys 4 times â€” want me to create a deploy skill?")
- **PM** â€” suggests skills when users repeatedly describe the same type of task

### 1.7 Why CodeButler Exists

**vs. Claude Code:** Slack-native. PM planning 100x cheaper. Automated memory. N parallel threads. Audit trail.
**vs. Cursor/Windsurf:** Fire-and-forget. No IDE needed. Team-native.
**vs. Devin/OpenHands:** Self-hosted. PM-mediated. Cost-transparent. Memory improves per thread.
**vs. Simple Slack bots:** They generate text. CodeButler ships code with PRs.

---

## 2. Slack Integration

### Concept Mapping (v1 â†’ v2)

| WhatsApp | Slack | Notes |
|----------|-------|-------|
| Group JID | Channel ID | Identifier |
| QR code pairing | OAuth App + Bot Token | Auth |
| whatsmeow events | Slack Socket Mode | Reception |
| `SendMessage` | `chat.postMessage` | Send text |
| Bot prefix `[BOT]` | Bot messages have `bot_id` | Native filtering |

### Slack App Setup

Bot token scopes: `channels:history`, `channels:read`, `chat:write`, `files:read`, `files:write`, `groups:history`, `groups:read`, `reactions:write`, `users:read`. Socket Mode enabled. Events: `message.channels`, `message.groups`. Tokens: Bot (`xoxb-...`) + App (`xapp-...`).

---

## 3. Installation

### Install Binary

```bash
go install github.com/leandrotocalini/codebutler/cmd/codebutler@latest
```

### `codebutler init`

Run `codebutler init` in a git repo. If the repo isn't configured, this is the only way to set it up â€” running `codebutler --role <any>` in an unconfigured repo tells you to run `init` first.

**Step 1: Global tokens** (only once per machine â€” `~/.codebutler/config.json` doesn't exist):

1. **Slack app** â€” guides you through creating the Slack app (scopes, Socket Mode, bot user). Asks for Bot Token (`xoxb-...`) + App Token (`xapp-...`)
2. **OpenRouter** â€” asks for API key (`sk-or-...`). Used for all LLM calls
3. **OpenAI** â€” asks for API key (`sk-...`). Used for image generation (Artist) and voice transcription (Whisper). **Required**
4. Saves all tokens to `~/.codebutler/config.json`

**Step 2: Repo setup** (once per repo â€” `<repo>/.codebutler/` doesn't exist):

1. **Seed `.codebutler/`** â€” creates folder, copies seed MDs (`pm.md`, `coder.md`, `reviewer.md`, `lead.md`, `artist.md`, `researcher.md`, `global.md`, `workflows.md`), copies seed skills (`skills/explain.md`, `test.md`, `changelog.md`, `hotfix.md`, `docs.md`, `security-scan.md`, `self-document.md`, `status.md`, `triage-issue.md`, `review-pr.md`, `release.md`), seeds `mcp.json` with GitHub MCP server (other servers commented as examples), creates `config.json` with default models, creates `artist/assets/`, `branches/`, `images/`
2. **Channel selection** â€” recommends creating `codebutler-<reponame>`. User can pick an existing channel or accept the recommendation
3. **`.gitignore`** â€” adds `.codebutler/branches/`, `.codebutler/images/` if not present
4. Saves channel to per-repo `config.json`

**Step 3: Service install** (once per machine):

1. Asks which agents to install on this machine (default: all 6). Different agents can run on different machines
2. Detects OS (macOS / Linux)
3. Installs selected services â€” one per agent, `WorkingDirectory=<repo>`, restart on failure:
   - macOS: LaunchAgent plists (`~/Library/LaunchAgents/codebutler.<repo>.<role>.plist`)
   - Linux: systemd user units (`~/.config/systemd/user/codebutler.<repo>.<role>.service`)
4. Starts selected services

**Subsequent repos:** Step 1 is skipped (tokens exist). Only steps 2-3 run. Same Slack app, different channel, new services.

**Subsequent machines:** Step 2 is skipped (`.codebutler/` already exists in git). Only steps 1 + 3 run. Same repo, different machine, different agents.

### `codebutler configure`

For post-init changes. Run `codebutler configure` in a configured repo.

- **Change Slack channel** â€” switch to a different channel
- **Add agent** â€” install a new agent service on this machine (e.g., add Coder to a more powerful machine)
- **Remove agent** â€” stop and uninstall an agent service from this machine
- **Update tokens** â€” change API keys
- **Show config** â€” display current config (machine + repo)

### Service Management

```bash
codebutler init               # first-time setup (tokens + repo + services)
codebutler configure           # change config post-init
codebutler start              # start all agents installed on this machine
codebutler stop               # stop all agents on this machine
codebutler status             # show running agents, active threads per agent
codebutler --role <role>      # run single agent in foreground (dev mode)
```

On machine reboot, all services restart automatically. If an agent crashes, the service manager restarts it. The agent reads active threads from Slack and picks up where it left off.

---

## 4. Config & Storage

### Config â€” Two Levels

**Global** (`~/.codebutler/config.json`) â€” secrets, never committed:
```json
{
  "slack": { "botToken": "xoxb-...", "appToken": "xapp-..." },
  "openrouter": { "apiKey": "sk-or-..." },
  "openai": { "apiKey": "sk-..." }
}
```

**Per-repo** (`<repo>/.codebutler/config.json`) â€” committed to git:
```json
{
  "slack": { "channelID": "C0123...", "channelName": "codebutler-myproject" },
  "models": {
    "pm": {
      "default": "moonshotai/kimi-k2",
      "pool": { "kimi": "moonshotai/kimi-k2", "claude": "anthropic/claude-sonnet-4-5-20250929", "gpt": "openai/gpt-4o-mini" }
    },
    "researcher": { "model": "moonshotai/kimi-k2" },
    "coder": { "model": "anthropic/claude-opus-4-6" },
    "reviewer": { "model": "anthropic/claude-sonnet-4-5-20250929" },
    "lead": { "model": "anthropic/claude-sonnet-4-5-20250929" },
    "artist": { "uxModel": "anthropic/claude-sonnet-4-5-20250929", "imageModel": "openai/gpt-image-1" }
  },
  "multiModel": {
    "models": [
      "anthropic/claude-opus-4-6",
      "google/gemini-2.5-pro",
      "openai/o3",
      "deepseek/deepseek-r1"
    ],
    "maxAgentsPerRound": 6,
    "maxCostPerRound": 1.00
  },
  "limits": { "maxConcurrentThreads": 3, "maxCallsPerHour": 100 }
}
```

All LLM calls route through OpenRouter. Agents needing multiple models define them explicitly (e.g., Artist has `uxModel` + `imageModel`). PM has a model pool for hot swap (`/pm claude`, `/pm kimi`).

**Multi-model config:** `multiModel.models` is the pool of models available for `MultiModelFanOut`. Any agent can use this pool â€” PM for brainstorming, Reviewer for multi-model code review, Coder when stuck, etc. `maxAgentsPerRound` caps how many parallel calls per round (default 6). `maxCostPerRound` is a soft limit â€” the calling agent estimates cost before fan-out and warns the user if it'll exceed.

**Default behavior if `multiModel` is not configured:** the pool auto-populates from all unique models already configured in `models.*`. If PM uses Kimi, Coder uses Opus, Reviewer uses Sonnet, and Artist uses Sonnet â€” the default pool is `[kimi, opus, sonnet]` (deduplicated). This means multi-model fan-out works out of the box without extra config â€” the user only needs to define `multiModel.models` explicitly if they want models in the pool that no agent uses as primary (e.g., Gemini, DeepSeek, o3).

### Storage â€” `.codebutler/` Folder

```
<repo>/.codebutler/
  config.json                    # Per-repo settings (committed)
  mcp.json                       # MCP server config â€” servers + per-agent access (committed)
  # Agent MDs â€” each is system prompt + project map + learnings
  pm.md                          # PM agent
  coder.md                       # Coder agent
  researcher.md                  # Researcher agent
  reviewer.md                    # Reviewer agent
  lead.md                        # Lead agent
  artist.md                      # Artist agent
  global.md                      # Shared project knowledge (all agents read)
  workflows.md                   # Process playbook
  artist/
    assets/                      # Screenshots, mockups, visual references
  skills/                        # Custom commands â€” one .md per skill (committed)
    explain.md                   # Seeded: explain how code works (PM)
    test.md                      # Seeded: write tests (Coder)
    changelog.md                 # Seeded: generate changelog from git (PM)
    hotfix.md                    # Seeded: investigate + quick-fix a bug (PM â†’ Coder)
    docs.md                      # Seeded: generate/update documentation (Coder)
    security-scan.md             # Seeded: security audit (Reviewer)
    self-document.md             # Seeded: document work in JOURNEY.md (Lead)
    status.md                    # Seeded: project status report (PM)
    triage-issue.md              # Seeded: triage GitHub issues (PM, GitHub MCP)
    review-pr.md                 # Seeded: review PRs with inline comments (Reviewer, GitHub MCP)
    release.md                   # Seeded: create GitHub releases (PM, GitHub MCP)
  research/                      # Researcher findings (committed, merged with PRs)
    stripe-api.md                # Example: Stripe best practices
    vite-plugins.md              # Example: Vite plugin system docs
  roadmap.md                     # Planned work items with status + dependencies
  branches/                      # Git worktrees, 1 per active thread (gitignored)
    <branchName>/                # One worktree per thread
      conversations/             # Agentâ†”model conversation files
        pm.json                  # PM's full model conversation for this thread
        coder.json               # Coder's full model conversation
        reviewer.json            # ...etc
  images/                        # Generated images (gitignored)
```

**Committed to git:** `config.json`, `mcp.json`, all `.md` files, `skills/`, `artist/assets/`, `research/`, `roadmap.md`. **Gitignored:** `branches/` (including conversation files), `images/`.

**Two layers of state:**
1. **Slack thread** â€” inter-agent messages + user interaction. The public record. Source of truth for what was communicated.
2. **Conversation files** (`conversations/<role>.json`) â€” agentâ†”model back-and-forth. Tool calls, results, reasoning, retries. Private to each agent. Lives in the worktree, dies with it.

OpenRouter is stateless â€” full message history sent on every call from the conversation file. On restart, agents scan active threads for unprocessed @mentions and resume model conversations from their JSON files.

---

## 5. Dependencies

**Remove** (from v1): `whatsmeow`, QR code libs.
**Add**: `github.com/slack-go/slack`, OpenRouter HTTP client, OpenAI HTTP client (image gen).
**Requires**: `gh` CLI (GitHub operations).

---

---

## 7. Inter-Agent Communication

**All inter-agent messages are Slack messages in the same thread.** No hidden bus. The user sees everything in real-time. The thread IS the source of truth.

### Agent Identities

One Slack bot app, six display identities: `@codebutler.pm`, `@codebutler.coder`, `@codebutler.reviewer`, `@codebutler.lead`, `@codebutler.researcher`, `@codebutler.artist`. Each posts with its own display name and icon.

### SendMessage(message, waitForReply)

LLM tool for posting to the current Slack thread. To direct a message to another agent, the LLM includes `@codebutler.<role>` in the message body â€” same as a human would. No `to` parameter; routing is handled by each process's message filter when it receives the Slack event.

**Sender identification:** the system automatically prefixes `@codebutler.<self>:` to the posted message. The LLM only writes the body â€” it doesn't need to identify itself. This guarantees consistent formatting and the receiving agent always knows who sent the message.

```json
{
  "name": "SendMessage",
  "parameters": {
    "message": "string â€” the text to post (include @codebutler.<role> to target an agent)",
    "waitForReply": "boolean â€” block the agent loop until a reply arrives in the thread"
  }
}
```

Example: PM calls `SendMessage(message: "@codebutler.coder implement: [plan]")` â†’ posted to Slack as `@codebutler.pm: @codebutler.coder implement: [plan]`.

The daemon only routes â€” **agents drive the flow themselves**.

### Conversation Examples

**PM â†” Coder:**
```
@codebutler.pm: @codebutler.coder implement this plan: [plan]
@codebutler.coder: @codebutler.pm the plan says REST but this project uses GraphQL. adapt?
@codebutler.pm: @codebutler.coder good catch, use GraphQL. here's the schema: [context]
```

**PM â†’ Artist:**
```
@codebutler.pm: @codebutler.artist feature: notification settings. requirements: [details]
@codebutler.artist: @codebutler.pm UX proposal:
  - layout: tabbed sections (channels, schedule, preview)
  - interaction: auto-save with toast confirmation
  - mobile: tabs collapse to accordion
```

**Coder â†’ Reviewer:**
```
@codebutler.coder: @codebutler.reviewer PR ready. branch: codebutler/add-notifications
@codebutler.reviewer: @codebutler.coder 3 issues: [security] executor.go:47, [test] missing edge case, [quality] duplicated handler
@codebutler.coder: @codebutler.reviewer fixed all 3, pushed
@codebutler.reviewer: @codebutler.coder approved âœ“
```

**Disagreement â†’ Lead:**
```
@codebutler.reviewer: @codebutler.lead disagreement on daemon.go:150 complexity
@codebutler.lead: Coder is right â€” state machines read better as one block. Add a comment.
```

### Escalation Hierarchy

```
User (final authority)
  â””â”€â”€ Lead (mediator, arbiter)
        â”œâ”€â”€ PM (orchestrator)
        â”œâ”€â”€ Coder (builder)
        â”œâ”€â”€ Reviewer (quality gate)
        â”œâ”€â”€ Researcher (web knowledge)
        â””â”€â”€ Artist (UI/UX design + images)
```

When two agents disagree â†’ Lead decides. **The user outranks everyone** â€” can jump in at any point, override any decision. The user IS the escalation.

---

## 8. Agent Architectures

Each agent is an independent OS process with its own Slack listener. All run the same binary. All execute tools locally. All communicate via Slack messages in the thread.

### Agent-Model Conversation (the temp file)

Each agent activation involves multiple round-trips with the model: tool calls, tool results, reasoning steps, retries. **Most of this never reaches Slack.** The full conversation lives in a JSON file per agent per thread:

```
.codebutler/branches/<branchName>/conversations/
  pm.json
  coder.json
  reviewer.json
  ...
```

**What's in the file:** system prompt, every user/assistant message, every tool call + result, every model response â€” the complete transcript of agentâ†”model interaction for that thread.

**What goes to Slack:** only the agent's curated output â€” the plan summary, the question to the user, the "PR ready" message. The agent decides what to post; the model's raw responses are not forwarded verbatim.

**Why this matters:**
- The Coder might make 20 tool calls (read files, write code, run tests, fix errors, retry) before posting "PR ready" in Slack. Those 20 rounds live in `coder.json`, not in the thread.
- On restart, the agent resumes from its conversation file â€” no context lost, no need to re-read the entire Slack thread.
- The Lead reads Slack thread messages for retrospective (what agents *said*), but could also read conversation files for deeper analysis (what agents *did*).

**Lifecycle:** created when the agent first processes a message in the thread. Lives in the worktree. Archived or deleted when the thread closes and worktree is cleaned up.

### PM â€” Always-Online Orchestrator

The entry point for all user messages. Talks to user, explores codebase, selects workflow or skill, delegates to other agents via @mentions in the thread. Cheap model (Kimi by default). System prompt: `pm.md` + `global.md` + `workflows.md` + skill index from `skills/`. Capped at 15 tool-calling iterations per activation.

The PM's goroutine for a thread stays alive while the Coder works â€” when the Coder @mentions PM with a question, the PM's Slack listener routes it to that thread's goroutine and responds.

### Researcher â€” On-Demand Web Research

Listens for @mentions from **any agent** (not just PM). Any agent can `@codebutler.researcher` when it needs external context â€” docs, best practices, API references, vulnerability databases, design patterns. Runs WebSearch + WebFetch â†’ synthesizes â†’ posts summary back in thread. Parallel-capable (multiple goroutines for concurrent research requests).

**Research persistence:** findings are saved to `.codebutler/research/` as individual MD files. When the Researcher persists a finding, it also adds a one-line entry to the `## Research Index` section in `global.md` using `@` references (e.g., `- Stripe API v2024 â€” @.codebutler/research/stripe-api-v2024.md`). Since all agents read `global.md` as part of their system prompt, they can see what research exists and read the full file when they need depth. Persisted research is committed to git and merges with PRs â€” knowledge accumulates across threads. The Researcher checks the index before searching again.

**The Researcher never acts on its own initiative.** It only activates when another agent asks. Its knowledge grows on-demand, driven by real questions from agents who encounter something they don't know.

System prompt: `researcher.md` + `global.md`.

### Artist â€” UI/UX Designer + Image Gen

Dual-model. Listens for @mentions from PM. Claude Sonnet for UX reasoning (layouts, component structure, UX flows). OpenAI gpt-image-1 for image gen/editing. Posts design proposals back in the thread. Reads `artist/assets/` for visual references to stay coherent with existing UI. System prompt: `artist.md` + `global.md`.

### Coder â€” Builder

Claude Opus 4.6. Listens for @mentions from PM (task) and Reviewer (feedback). Full tool set, executes locally in isolated worktree. Creates PRs. When it needs context, @mentions PM in the thread. When done, @mentions Reviewer. System prompt: `coder.md` + `global.md` + task context from thread.

**Sandboxing:** MUST NOT install packages, leave worktree, modify system files, or run destructive commands. Enforced at tool execution layer (path validation, command filtering) â€” stronger than prompt-only.

### Reviewer â€” Code Review Loop

Listens for @mentions from Coder ("PR ready"). Checks: code quality, security (OWASP), test coverage, consistency, plan compliance. Sends structured feedback back to Coder via @mention. Loop until approved (max 3 rounds). When approved, @mentions Lead. Disagreements escalate to Lead. System prompt: `reviewer.md` + `global.md`.

### Lead â€” Mediator + Retrospective

Listens for @mentions from Reviewer ("approved") or from agents in disagreement. At thread close, reads **full thread transcript** from Slack. Three phases:

1. **Analysis** (solo) â€” identifies friction, wasted turns, escalation patterns
2. **Discussion** (multi-agent) â€” @mentions each agent in the thread, discusses improvements
3. **Proposals** (to user) â€” concrete updates to agent MDs, `global.md`, `workflows.md`

**Produces:** PR description, learnings for agent MDs, workflow evolution, usage report.

**Workflow evolution** â€” add step, create new workflow, or automate a step. Built collaboratively with agents during discussion.

**The flywheel:** rough workflow â†’ friction â†’ Lead discusses â†’ improvement â†’ user approves â†’ smoother next thread.

System prompt: `lead.md` + `global.md` + `workflows.md`. Turn budget configurable.

---

## 9. Message Flow

No state machine. Slack threads provide natural conversation boundaries. Each agent process handles its own events independently.

### How a Task Flows

```
User posts in Slack thread
  â†’ PM creates worktree + starts conversation file
  â†’ PM plans, explores, proposes
  â†’ PM posts: "@codebutler.coder implement: [plan]"
  â†’ Coder implements in worktree (its own conversation file there too)
  â†’ Coder posts: "@codebutler.pm what auth method?" (question)
  â†’ PM responds
  â†’ Coder posts: "@codebutler.reviewer PR ready: [branch]"
  â†’ Reviewer reads diff, posts feedback
  â†’ (loop until approved)
  â†’ Reviewer posts: "@codebutler.lead review done"
  â†’ Lead reads full thread, runs retrospective
```

Every step is a Slack message. Agents drive the flow themselves via @mentions.

### Thread Phases

- **`pm`** â€” PM planning. If feature has UI â†’ @mentions Artist
- **`coder`** â€” Coder working in worktree. PM available for questions
- **`review`** â€” Reviewer â†” Coder feedback loop
- **`lead`** â€” Lead retrospective
- **`closed`** â€” PR merged, worktree cleaned

---

## 10. Memory System

### One File Per Agent = System Prompt + Memory

| File | What it holds | Who updates it |
|------|--------------|---------------|
| `pm.md` | Personality + rules. **Project map:** features, domains. **Learnings:** interview techniques, what Coder needs | Lead + user |
| `coder.md` | Personality + rules. Tool defs, sandbox. **Project map:** architecture, patterns. **Learnings:** coding patterns | Lead + user |
| `reviewer.md` | Personality + rules. Review checklist. **Project map:** quality hotspots. **Learnings:** recurring issues | Lead + user |
| `lead.md` | Personality + rules. Retrospective structure. **Project map:** efficiency patterns. **Learnings:** mediation strategies | Lead + user |
| `artist.md` | Personality + rules. Design guidelines. **Project map:** UI components, screens, design system. **Learnings:** what Coder needs | Lead + user |
| `researcher.md` | Personality + rules. Search strategies. **Research index:** what it has investigated, where findings are stored | Lead + self |
| `research/` | Persisted research findings as individual MD files. Committed to git, merges with PRs. Researcher decides what to persist | Researcher |
| `global.md` | Shared project knowledge: architecture, tech stack, conventions, deployment | Lead + user |
| `workflows.md` | Process playbook: step-by-step workflows per task type | Lead + user |
| `artist/assets/` | Screenshots, mockups, visual references | Artist + Lead |

**Learnings only go where needed.** If the Reviewer didn't participate, its MD doesn't change. User approves what gets saved.

### workflows.md â€” Process Playbook

Seeded on first run:

```markdown
## implement
1. PM: create worktree + conversation file
2. PM: classify as implement
3. PM: interview user (acceptance criteria, edge cases, constraints)
4. PM: explore codebase (integration points, patterns)
5. PM: if unfamiliar tech â†’ Researcher: docs, best practices
6. PM: if UI component â†’ Artist: design UI/UX. Artist returns proposal
7. PM: propose plan (file:line refs, Artist design if applicable)
8. User: approve
9. Coder: implement in worktree (PM plan + Artist design as input)
10. Coder: create PR
11. Reviewer: review diff (quality, security, tests, plan compliance)
12. Reviewer: if issues â†’ Coder fixes â†’ re-review
13. Reviewer: approved
14. Lead: retrospective (discuss with agents, propose learnings)
15. User: approve learnings, merge

## discovery
1. PM: classify as discovery
2. PM: structured discussion (goals, constraints, priorities, user stories)
3. PM: if needs external context â†’ Researcher
4. PM: if UI features â†’ Artist: propose UX flows
5. PM: produce proposals (summary, user story, criteria, Artist design, complexity, dependencies)
6. User: approve proposals
7. PM â†’ Lead: hand off
8. Lead: create roadmap (priority, dependencies, milestones)
9. User: approve roadmap
10. Lead: create GitHub issues or commit roadmap
11. Lead: retrospective

Each roadmap item â†’ future implement thread. Start: manually, "start next", or "start all".

## bugfix
1. PM: find relevant code, root cause hypothesis
2. PM: if external API â†’ Researcher
3. PM: propose fix plan
4. User: approve
5. Coder: fix + regression test
6. Reviewer: review â†’ loop
7. Lead: retrospective

## question
1. PM: explore codebase, answer directly
2. PM: if needs context â†’ Researcher
3. (No Coder, no Reviewer, no Lead â€” unless user escalates)

## refactor
1. PM: analyze code, propose before/after
2. User: approve
3. Coder: refactor, ensure tests pass
4. Reviewer: review â†’ loop
5. Lead: retrospective
```

### Memory Extraction (Lead)

After PR creation, Lead proposes updates routed to the right file:
- Architecture decisions, shared conventions â†’ `global.md`
- Workflow refinements, new workflows, automations â†’ `workflows.md`
- Agent-specific learnings â†’ the relevant agent's MD
- New UI screenshots â†’ `artist/assets/`
- Coding conventions â†’ `coder.md`
- Roadmap status updates â†’ `roadmap.md`

**Project maps evolve:** when a thread adds a screen, changes an API, or introduces a pattern, the Lead updates the relevant agent's project map. User approves.

### Learning Patterns

**Message-driven:** Coder keeps asking PM about auth â†’ Lead proposes workflow step for auth check â†’ no question next time.

**Inter-agent:** Each agent's MD accumulates how to work with other agents. PM learns what Coder needs. Artist learns what detail level Coder expects. Cross-cutting knowledge goes to `global.md`.

### Git Flow

All MDs follow PR flow: Lead proposes â†’ user approves â†’ committed to PR branch â†’ lands on main with merge. Git IS the knowledge transport.

---

## 11. Thread Lifecycle

### 1 Thread = 1 Branch = 1 PR

Non-negotiable. Only the user closes a thread. No timeouts.

### After PR Creation
1. Coder â†’ Reviewer: "PR ready" (agent-driven handoff)
2. Reviewer â†” Coder: review loop until approved
3. Reviewer â†’ Lead: "approved"
4. Lead: retrospective, proposes learnings â†’ user approves â†’ commit to PR branch

### On User Close
- Lead: PR summary via `gh pr edit`, usage report, journal finalization
- Merge PR (`gh pr merge --squash`)
- Delete remote branch, remove worktree
- Notify overlapping threads to rebase

---

## 12. Conflict Coordination

**Detection:** file overlap, directory overlap, semantic overlap (PM analyzes). Checked at thread start and after each Coder response.

**Merge order:** PM suggests smallest-first when multiple PRs touch overlapping files.

**Post-merge:** PM notifies other active threads to rebase.

---

## 13. Worktree Isolation & Git Sync

Each thread gets a git worktree in `.codebutler/branches/<branchName>/`. **Created early by the PM** â€” as soon as the PM starts working on a thread, it creates the worktree, pushes the branch to remote, and begins saving its conversation file there. Branch: `codebutler/<slug>`.

### Git Sync Protocol

**Every change an agent makes to the worktree must be committed and pushed.** This is non-negotiable â€” it ensures all agents (whether on the same machine or different machines) see the latest state.

1. **After any file change** (code, MDs, conversation files, research): `git add` + `git commit` + `git push`
2. **Before reading shared state** (agent MDs, global.md, research/): `git pull` to get the latest
3. **On branch creation** (PM starts a thread): create worktree, push branch to remote immediately
4. **When an agent is @mentioned for a branch it doesn't have locally**: pull the branch, create local worktree, start working

### Divergence Handling

If two agents push to the same branch concurrently (e.g., Coder pushed code while Researcher pushed a research file):

1. The second push fails (non-fast-forward)
2. Agent pulls with rebase: `git pull --rebase`
3. If conflicts: agent resolves automatically (each agent knows its own files â€” conversation files never conflict, agent MDs rarely conflict)
4. If unresolvable: agent posts in thread asking for help, marks the issue

In practice, conflicts are rare because agents work on different files: Coder writes code, Researcher writes to `research/`, Lead writes to MDs. The main risk is two agents editing the same MD â€” the Lead handles most MD writes, so this is serialized naturally.

### Distributed Agents

Agents can run on different machines. The default is all 6 on one machine, but the architecture supports distribution:

- **Same machine (default):** all agents share the filesystem. Git sync still applies â€” agents commit and push after every change. This is the simplest setup and works for most cases
- **Multiple machines:** each machine runs a subset of agents (e.g., Coder on a powerful GPU machine, PM on a cheap always-on server). Each machine has its own clone of the repo. Agents coordinate through git (code/files) and Slack (messages)

**What changes with distribution:**
- `codebutler init` on each machine â€” same repo, same Slack app, different agents installed as services
- Each machine creates local worktrees for active branches on demand (pull from remote)
- Conversation files are per-agent, so no conflicts â€” each agent owns its own `conversations/<role>.json`
- The PM pushes the branch immediately after creation so other agents on other machines can pull it

**What doesn't change:** Slack is still the communication bus. @mentions still drive the flow. The agent loop is the same. The only difference is that "read file" might require a `git pull` first.

| Platform | Init | Build Isolation |
|----------|------|-----------------|
| iOS/Xcode | `pod install` | `-derivedDataPath .derivedData` |
| Node.js | `npm ci` | `node_modules/` per worktree |
| Go | Nothing | Module cache is global |
| Python | `venv + pip install` | `.venv/` per worktree |
| Rust | Nothing | `CARGO_TARGET_DIR=.target` |

---

## 14. Multi-Model Orchestration

All via OpenRouter. PM has model pool with hot swap. Cost controls: per-thread cap, per-day cap, per-user hourly limit. Circuit breaker: 3x PM failure â†’ fallback for 5 minutes.

### Brainstorm: Dynamic Thinker Agents

The `brainstorm` workflow introduces a new pattern: the PM dynamically creates lightweight "Thinker" agents â€” ephemeral single-shot LLM calls that run in parallel inside the PM process.

**What Thinkers are:**
- Goroutines inside the PM process, not separate OS processes
- Single-shot OpenRouter calls â€” no tools, no agent loop, no conversation persistence
- Each gets a custom system prompt crafted by the PM for that specific brainstorm
- Each uses a different model from the `multiModel.models` pool
- They respond, the PM collects, they die

**What Thinkers are NOT:**
- Not CodeButler agents (no Slack presence, no @mention, no conversation file)
- Not pre-configured personas (PM creates them dynamically per topic)
- Not interactive (one prompt in, one response out â€” no follow-ups)

**How it works at the code level:**

```go
type ThinkerConfig struct {
    Name         string // e.g., "Quantitative Analyst"
    SystemPrompt string // PM-generated, unique per Thinker
    Model        string // OpenRouter model ID from multiModel.models
}

type ThinkerResult struct {
    Name     string
    Model    string
    Response string
    Tokens   TokenUsage
    Duration time.Duration
}

// PM calls this tool during brainstorm workflow
func (pm *PM) MultiModelFanOut(ctx context.Context, thinkers []ThinkerConfig, userPrompt string) []ThinkerResult {
    results := make([]ThinkerResult, len(thinkers))
    g, ctx := errgroup.WithContext(ctx)
    for i, t := range thinkers {
        g.Go(func() error {
            resp, err := pm.provider.ChatCompletion(ctx, ChatRequest{
                Model: t.Model,
                Messages: []Message{
                    {Role: "system", Content: t.SystemPrompt},
                    {Role: "user", Content: userPrompt},
                },
            })
            if err != nil {
                results[i] = ThinkerResult{Name: t.Name, Model: t.Model, Response: "Error: " + err.Error()}
                return nil // don't fail the group â€” other Thinkers continue
            }
            results[i] = ThinkerResult{
                Name:     t.Name,
                Model:    t.Model,
                Response: resp.Content,
                Tokens:   resp.Usage,
                Duration: resp.Duration,
            }
            return nil
        })
    }
    g.Wait()
    return results
}
```

**PM as creative director.** The PM's intelligence is what makes this work. It doesn't just forward a prompt to N models â€” it designs each Thinker's persona to maximize diversity of thought:

- Analyzes the topic and identifies what expert perspectives would be most valuable
- Crafts genuinely different system prompts (not variations of "be a senior engineer")
- Includes domain-specific context in each prompt (codebase state, constraints, research)
- Assigns models strategically â€” reasoning models (o3, DeepSeek-R1) for architectural questions, creative models (Claude, Gemini) for product ideation. **Each Thinker MUST use a different model** â€” no duplicates in a single round. Model diversity is the core value proposition; same model twice with different prompts is redundant. The number of Thinkers is capped by the number of available models in the pool
- After collecting responses, synthesizes across all Thinkers â€” finding patterns, conflicts, and unique insights

**Cost awareness.** Each brainstorm round costs N Ã— single-shot calls. The PM estimates cost before fan-out (model pricing Ã— estimated tokens) and posts it in the thread. If it exceeds `multiModel.maxCostPerRound`, the PM asks the user before proceeding. Thinker costs are tracked in `ThreadCost` and appear in the Lead's usage report.

**Error handling.** If a Thinker's model fails (rate limit, timeout, content filter), the other Thinkers continue â€” one failure doesn't cancel the round. The PM notes the failure in the synthesis ("DeepSeek-R1 was unavailable this round â€” 3 of 4 Thinkers responded"). The circuit breaker is per-model, so a failing brainstorm model doesn't affect the agents' primary models.

---

---

## 16. Operational Details

### Slack Features
- Agent identity: one bot, six display names + icons
- Reactions: ðŸ‘€ processing, âœ… done
- Threads = sessions (1:1). Multiple concurrent
- Code snippets: <20 lines inline, â‰¥20 lines as file uploads

### Logging
Structured tags: INF, WRN, ERR, DBG, MSG, PM, RSH, CLD, LED, IMG, RSP, MEM, AGT. Ring buffer + SSE for web dashboard.

### PR Description
Lead generates summary at close via `gh pr edit`. Thread journal (`.codebutler/journals/thread-<ts>.md`) captures tool-level detail not visible in Slack.

### Error Recovery

Each process is independent â€” one crash doesn't affect others.

| Failure | Recovery |
|---------|----------|
| Agent process crashes | Service restarts it. Reads active threads from Slack, processes unresponded @mentions |
| Slack disconnect | Auto-reconnect per process (SDK handles) |
| LLM call hangs | context.WithTimeout per goroutine â†’ kill, reply error in thread |
| LLM call fails | Error reply in thread, session preserved for retry |
| Agent not running | @mention sits in thread. When agent starts, it reads thread and processes |
| Machine reboot | All 6 services restart, each reads active threads from Slack |

### Access Control
Channel membership = access. Optional: allowed users, max concurrent threads, hourly/daily limits.

---

## 15. Learn Workflow â€” Onboarding & Re-learn

### First Run (Automatic)

When `.codebutler/` is seeded for the first time on a repo that already has code (detected: files exist outside `.codebutler/`), the learn workflow triggers automatically after the wizard completes. A Slack thread is created in the project channel for the learn session.

### Re-learn (Manual or Suggested)

- **Manual:** user posts "re-learn" or "refresh knowledge" in the channel
- **Suggested:** the Lead proposes a re-learn when it detects project maps are significantly out of sync with the codebase (after many threads, major refactors, etc.)

### How It Works

All agents participate in parallel in a single thread, each exploring the codebase from their own perspective:

| Agent | What it explores | What it populates |
|-------|-----------------|-------------------|
| **PM** | Project structure, README, entry points, features, domains | PM project map |
| **Coder** | Architecture, patterns, conventions, build system, test framework, dependencies | Coder project map |
| **Reviewer** | Test coverage, CI config, linting, security patterns, quality hotspots | Reviewer project map |
| **Artist** | UI components, design system, styles, screens, responsive patterns | Artist project map |
| **Lead** | Reads what all agents found, synthesizes shared knowledge | `global.md` |
| **Researcher** | Nothing on its own â€” purely reactive, responds when other agents ask | `research/` folder |

**Each agent is intelligent about what to read.** No fixed budget or exploration rules. The PM reads entry points and READMEs, not test files. The Coder reads core modules and build config, not docs. Each agent's personality and perspective naturally guides what's relevant.

**Researcher is purely reactive.** It does not explore on its own. Other agents @mention it on demand when they encounter something they need external context for:
- PM finds Stripe integration â†’ `@codebutler.researcher Stripe API best practices?`
- Coder sees unfamiliar framework â†’ `@codebutler.researcher Vite 6 plugin system docs?`
- Reviewer sees custom linter â†’ `@codebutler.researcher eslint-config-airbnb rules?`

The Researcher's knowledge accumulates organically, driven by real questions â€” not a dump of everything that might be useful.

### Output

Each agent populates their **project map** section in their MD file. The Lead populates `global.md` with shared knowledge (architecture, tech stack, conventions, key decisions). The user reviews and approves all changes before they're committed.

### Re-learn vs Incremental Learning

| Type | When | What happens |
|------|------|-------------|
| **Incremental** | After each thread (default) | Lead updates specific project maps surgically. Small, targeted changes |
| **Re-learn** | On demand or suggested | Full refresh. Agents re-read the codebase, compare with existing knowledge, **compact**: remove outdated info, update what changed, add what's new. Result is cleaner and possibly smaller â€” not just additive |

Re-learn is knowledge garbage collection. The project maps should reflect the project as it is now, not as it was plus every change that ever happened.

---

## 17. Roadmap System & Unattended Development

### Roadmap File

The roadmap lives in `.codebutler/roadmap.md` â€” a committed file in the repo. Source of truth for planned work. The PM updates status as threads execute. The user can read and edit it directly at any time.

```markdown
# Roadmap: [project/feature set name]

## 1. Auth system
- Status: done
- Branch: codebutler/auth-system
- Depends on: â€”
- Acceptance criteria: JWT-based auth, login/register endpoints, middleware

## 2. User profile API
- Status: in_progress
- Branch: codebutler/user-profile
- Depends on: 1
- Acceptance criteria: CRUD endpoints, avatar upload, validation

## 3. Profile UI
- Status: pending
- Depends on: 1, 2
- Acceptance criteria: Profile page, edit form, avatar picker

## 4. Notification system
- Status: pending
- Depends on: â€”
- Acceptance criteria: Email + push, per-user preferences, queue-based
```

Statuses: `pending`, `in_progress`, `done`, `blocked`.

If the user wants GitHub issues at any point, the Lead can generate them from the roadmap â€” but the roadmap file remains the source of truth.

### Three Thread Types

#### 1. Add to Roadmap (`roadmap-add`)

User starts a thread to define new features. PM interviews the user (structured discovery), Artist proposes UX if needed, and the result is new items added to `roadmap.md`. No code, no worktree, no PR.

This can happen multiple times â€” the roadmap grows incrementally. Each thread adds items. The user can also edit the file directly.

#### 2. Implement Roadmap Item (`roadmap-implement`)

User starts a thread and references a roadmap item: "implement item 3" or describes it. PM picks the item from the roadmap, runs the standard `implement` workflow. On completion, PM marks the item as `done` in the roadmap and updates the branch name.

Same as a regular implement thread, but the plan comes from the roadmap item's acceptance criteria instead of a fresh interview.

#### 3. Implement All (`develop`)

User starts a thread and says "start all", "develop everything", or "implement the roadmap". PM orchestrates **unattended execution** of all `pending` items in the roadmap:

1. Reads `roadmap.md`, builds a dependency graph
2. Launches independent items in parallel (respecting `maxConcurrentThreads` from config)
3. Creates a **new Slack thread for each item** (1 thread = 1 branch = 1 PR, non-negotiable)
4. When an item completes (`done`), checks if dependent items are now unblocked â†’ launches them
5. Posts periodic status updates in the **orchestration thread** (the original thread where the user said "start all")

The orchestration thread is the dashboard. The PM posts updates there:
```
Roadmap progress:
âœ… 1. Auth system â€” done (PR #12 merged)
ðŸ”„ 2. User profile API â€” in progress
ðŸ”„ 4. Notification system â€” in progress
â³ 3. Profile UI â€” waiting on 1, 2
```

### Unattended Execution Model

In `develop` mode, the PM has autonomy to approve plans within the scope of the roadmap. **The roadmap IS the approval.** The user approved the acceptance criteria when they approved the roadmap â€” the PM doesn't need to ask again for each item.

The PM:
- Creates the plan following the roadmap item's acceptance criteria
- Hands off to Coder directly (skips user approval step)
- Only escalates to user when something is genuinely ambiguous, blocked, or outside the roadmap's scope

### User Tagging

When any agent needs user input during unattended execution (or any thread):

1. Read the Slack thread history
2. Extract all Slack user IDs that have participated in the thread (posted messages, not just reactions)
3. Tag them in the message requesting input

```
@user1 @user2 â€” blocked on item 3: the roadmap says "JWT auth" but the existing
codebase uses session cookies. Should I migrate to JWT or adapt the plan to sessions?
```

This ensures the right people get notified even when they're not actively watching. The thread creator is always included.

### Failure Handling

- If a thread blocks (needs user input, unfixable test failures, etc.), other independent items continue executing
- Items that depend on a blocked item are marked `blocked` in the roadmap with a reason
- PM posts a summary in the orchestration thread: "item 3 blocked â€” needs user input on auth approach. Items 4, 5 continuing. Item 6 waiting on 3."
- When the user unblocks an item (answers the question), the PM resumes automatically

---

---

## 18. Migration Path

1. Slack client + messaging (replace WhatsApp)
2. Thread dispatch + worktrees (replace state machine)
3. OpenRouter + native agent loop (replace `claude -p`)
4. PM tools + memory system
5. Artist integration + image/UX flow
6. Conflict detection + merge coordination

---

## 19. Decisions

- [x] **Separate OS processes** â€” one per agent, each with its own Slack listener, goroutines per thread
- [x] **Communication 100% via Slack** â€” no IPC, no RPC. Tasks are @mentions in the thread
- [x] **No database** â€” Slack thread is the source of truth. No SQLite. OpenRouter is stateless
- [x] **OpenAI-compatible tool calling** â€” OpenRouter normalizes all models to OpenAI tool calling format
- [x] **Each agent executes tools locally** â€” no RPC to a central executor
- [x] **All agents always running** â€” idle until @mentioned, pick up pending messages on restart
- [x] Multi-agent architecture â€” one binary, parameterized by `--role`
- [x] One MD per agent = system prompt + project map + learnings (seeded on first run, evolved by Lead)
- [x] `global.md` â€” shared project knowledge for all agents
- [x] `workflows.md` â€” process playbook, evolved by Lead
- [x] OpenRouter for all LLM calls (no CLI dependency)
- [x] Native tools in Go (same as Claude Code + more)
- [x] Artist as UI/UX designer â€” dual-model (Sonnet for UX, OpenAI for images). Artist output + PM plan = Coder input
- [x] Reviewer agent â€” code review loop after Coder, before Lead
- [x] Thread is source of truth â€” all inter-agent messages visible in Slack thread
- [x] Agent identities â€” `@codebutler.pm`, `@codebutler.coder`, etc. One bot, six identities
- [x] Agent-driven flow â€” agents pass work via @mentions, daemon only routes
- [x] Escalation hierarchy â€” user > Lead > individual agents
- [x] Discovery workflow â€” PM interviews â†’ Artist designs â†’ Lead builds roadmap
- [x] Escalation-driven learning â€” questions today â†’ workflow improvements tomorrow
- [x] Project map per agent â€” each knows the project from its perspective
- [x] Artist visual memory â€” `artist/assets/` for screenshots, mockups
- [x] Thread = Branch = PR (1:1:1, non-negotiable)
- [x] User closes thread explicitly (no timeouts)
- [x] Worktree isolation, per-platform init
- [x] Git flow for all MDs â€” learnings land on main with merge
- [x] PM model pool with hot swap
- [x] `gh` CLI for GitHub operations
- [x] Goroutine-per-thread, buffered channels, panic recovery
- [x] **`codebutler init` + `codebutler configure`** â€” explicit init command for first setup (tokens + repo + services). Configure for post-init changes (channel, add/remove agents, tokens)
- [x] **OpenAI key mandatory** â€” required for image generation (Artist) and voice transcription (Whisper). OpenRouter can't generate images
- [x] **OS services with auto-restart** â€” LaunchAgent (macOS) / systemd (Linux). 6 services per repo. Survive reboots, restart on crash
- [x] **Multi-repo = same Slack app, different channels** â€” global tokens shared, per-repo config separate
- [x] **Agentâ†”model conversation files** â€” per-agent, per-thread JSON in worktree. Full model transcript (tool calls, reasoning, retries) separate from Slack messages. Agent decides what to post publicly
- [x] **Learn workflow** â€” automatic onboarding on first run (repo with existing code), re-learn on demand or suggested by Lead. Each agent explores from its perspective, Researcher reactive on demand. Compacts knowledge on re-learn
- [x] **Roadmap file** â€” `.codebutler/roadmap.md`, committed to git. Source of truth for planned work. Not GitHub issues (can generate them optionally)
- [x] **Three roadmap thread types** â€” add items (discovery â†’ roadmap), implement one item, implement all (unattended batch)
- [x] **Unattended develop** â€” PM orchestrates all roadmap items, approves plans within roadmap scope, only escalates when genuinely blocked
- [x] **Researcher open access** â€” any agent can @mention Researcher, not just PM. Research persisted in `.codebutler/research/`, committed to git, merges with PRs
- [x] **User tagging** â€” when agents need input, tag all users who participated in the thread
- [x] **Git sync protocol** â€” every file change is committed + pushed. Agents pull before reading shared state. Non-negotiable for distributed support
- [x] **Distributed agents** â€” agents can run on different machines. Same repo, same Slack, different services. Git + Slack as coordination. Default is all on one machine
- [x] **PM workflow menu** â€” when user intent is ambiguous, PM presents available workflows as options. Teaches new users what CodeButler can do
- [x] **Research Index in global.md** â€” Researcher adds `@` references to persisted findings in global.md. All agents see what research exists via their system prompt
- [x] **MCP support with per-agent access** â€” `.codebutler/mcp.json` defines MCP servers + which roles can use them. Agent process only launches servers assigned to its role. MCP tools appear alongside native tools in the agent loop. Secrets via env vars, never in config
- [x] **Skills â€” custom commands** â€” `.codebutler/skills/*.md` defines project-specific reusable commands. More atomic than workflows, single-agent focused. PM matches trigger phrases during intent classification. Variables captured from user message. Created by team, Lead proposes from retrospective patterns
- [x] **No vector DB for memory** â€” MD-based memory with Lead curation, git versioning, and full-context loading is sufficient. Agent MDs are small enough to load entirely into context (100% recall, no retrieval errors). Vector DB adds infrastructure, latency, embedding costs, and breaks distributed agents (which coordinate via git + Slack only). Revisit if MDs grow beyond ~50K tokens or cross-project knowledge transfer is needed

---

## 20. v1 vs v2

| Aspect | v1 (WhatsApp) | v2 (Slack + OpenRouter) |
|--------|---------------|------------------------|
| Platform | WhatsApp (whatsmeow) | Slack (Socket Mode) |
| LLM execution | `claude -p` subprocess | OpenRouter API (native loop) |
| Tools | Delegated to Claude Code | Owned by CodeButler |
| System prompts | CLAUDE.md | Per-agent MDs (= prompt + memory) + global.md + workflows.md |
| Parallelism | 1 conversation | N concurrent threads |
| State machine | ~300 lines, 4 states | None (event-driven) |
| Isolation | Shared directory | Git worktrees |
| Agents | 1 (Claude) | 6 (PM, Researcher, Artist, Coder, Reviewer, Lead) |
| Communication | N/A | Inter-agent messaging in thread |
| Memory | None | Per-agent, git flow, Lead-extracted |
| Team support | Single user | Multi-user native |
| Code complexity | ~630 lines daemon.go | ~200 lines estimated |
